import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import urllib3
import ssl
import json
import re
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util import ssl_
from selenium.webdriver.common.by import By
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support import expected_conditions as EC

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX (Gi·ªØ nguy√™n t·ª´ c√°c bot tr∆∞·ªõc) ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_kdh_news(seen_ids):
    """
    H√†m c√†o Khang ƒêi·ªÅn (KDH).
    - URL 1: B√°o c√°o & C√°o b·∫°ch (L·ªçc l·∫•y BCTC).
    - URL 2: ƒêHƒêCƒê (X·ª≠ l√Ω layout n·∫±m ngang).
    """
    
    current_year = datetime.now().year
    
    configs = [
        {
            "name": "BCTC & C√°o b·∫°ch",
            "url": "https://www.khangdien.com.vn/co-dong/bao-cao-cao-bach",
            "type": "BCTC", # ƒê√°nh d·∫•u ƒë·ªÉ l·ªçc t·ª´ kh√≥a
            "selector": "li" # B√™n BCTC n√≥ n·∫±m trong th·∫ª li
        },
        {
            "name": "ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng",
            "url": "https://www.khangdien.com.vn/co-dong/dai-hoi-dong-co-dong",
            "type": "AGM",
            "selector": ".stockcol" # B√™n ƒêHƒêCƒê n√≥ n·∫±m trong div class stockcol (layout ngang)
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    
    # Setup Session
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t KDH (NƒÉm {current_year}) ---")

    for config in configs:
        try:
            response = session.get(config["url"], headers=headers, timeout=20, verify=False)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Ch·ªçn c√°c ph·∫ßn t·ª≠ ch·ª©a tin d·ª±a tr√™n config
            items = soup.select(config["selector"])
            
            count_in_page = 0
            
            for item in items:
                # 1. T√åM NG√ÄY TH√ÅNG (Quan tr·ªçng nh·∫•t)
                # D·ª±a tr√™n ·∫£nh: Ng√†y n·∫±m trong th·∫ª <i>(29/10/2025)</i>
                date_tag = item.select_one('i')
                if not date_tag: continue
                
                raw_date_text = date_tag.get_text(strip=True)
                
                # D√πng Regex ƒë·ªÉ b·∫Øt chu·ªói dd/mm/yyyy n·∫±m trong ngo·∫∑c ƒë∆°n
                match = re.search(r'(\d{2}/\d{2}/\d{4})', raw_date_text)
                if not match: continue
                
                date_str = match.group(1)
                
                try:
                    pub_date = datetime.strptime(date_str, "%d/%m/%Y")
                    # L·ªåC NƒÇM: Ch·ªâ l·∫•y nƒÉm hi·ªán t·∫°i
                    if pub_date.year != current_year:
                        continue
                except:
                    continue

                # 2. T√åM LINK & TITLE
                a_tag = item.select_one('a')
                if not a_tag: continue
                
                link = a_tag.get('href')
                title = a_tag.get_text(strip=True) or a_tag.get('title')
                
                if not link or not title: continue
                
                # 3. L·ªåC T·ª™ KH√ìA (Ch·ªâ √°p d·ª•ng cho m·ª•c BCTC nh∆∞ y√™u c·∫ßu)
                if config["type"] == "BCTC":
                    title_lower = title.lower()
                    # Ch·ªâ l·∫•y n·∫øu ti√™u ƒë·ªÅ ch·ª©a c√°c t·ª´ kh√≥a t√†i ch√≠nh
                    keywords = ["bctc", "b√°o c√°o t√†i ch√≠nh", "financial", "l·ª£i nhu·∫≠n", "so√°t x√©t", "ki·ªÉm to√°n"]
                    if not any(kw in title_lower for kw in keywords):
                        continue

                # 4. CHU·∫®N H√ìA LINK
                if not link.startswith('http'):
                    link = f"https://www.khangdien.com.vn{link}"
                    
                # 5. CHECK TR√ôNG
                news_id = link
                if news_id in seen_ids: continue
                if any(x['id'] == news_id for x in new_items): continue

                new_items.append({
                    "source": f"KDH - {config['name']}",
                    "id": news_id,
                    "title": title,
                    "date": date_str,
                    "link": link
                })
                count_in_page += 1
                
            time.sleep(1)

        except Exception as e:
            print(f"[KDH] L·ªói t·∫°i {config['name']}: {e}")
            continue

    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_vix_news(seen_ids):
    """
    H√†m c√†o Ch·ª©ng kho√°n VIX.
    - URL 1 (BCTC): D√πng Tab (#menu2025).
    - URL 2 (ƒêHƒêCƒê): D√πng B·∫£ng tr·ª±c ti·∫øp (#tblPublish).
    """
    
    current_year = datetime.now().year
    
    sources = [
        {
            "name": "BCTC",
            "url": "https://vixs.vn/bao-cao",
            "type": "GRID_TAB" # Lo·∫°i 1: B·∫£ng Grid n·∫±m trong Tab
        },
        {
            "name": "ƒêHƒêCƒê",
            "url": "https://vixs.vn/qhcd/dai-hoi-co-dong",
            "type": "DIRECT_TABLE" # Lo·∫°i 2: B·∫£ng tr·ª±c ti·∫øp, kh√¥ng c√≥ Tab nƒÉm
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t VIX (NƒÉm {current_year}) ---")

    for source in sources:
        try:
            # print(f"   >> ƒêang t·∫£i: {source['name']}...")
            response = session.get(source["url"], headers=headers, timeout=20, verify=False)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # --- X·ª¨ L√ù URL 1: BCTC (GRID trong TAB) ---
            if source["type"] == "GRID_TAB":
                # T√¨m Tab nƒÉm hi·ªán t·∫°i
                year_tab_id = f"menu{current_year}"
                year_content = soup.find(id=year_tab_id)
                
                if not year_content: continue
                
                table = year_content.find('table')
                if not table: continue
                
                # L·∫•y Header c·ªôt
                headers_text = [th.get_text(strip=True) for th in table.select('thead th')]
                
                # Duy·ªát d√≤ng
                for tr in table.select('tbody tr'):
                    cells = tr.find_all('td')
                    if not cells: continue
                    
                    row_title = cells[0].get_text(strip=True)
                    
                    # Duy·ªát c√°c √¥ Qu√Ω
                    for i, cell in enumerate(cells[1:], start=1):
                        a_tag = cell.find('a')
                        if not a_tag: continue
                        
                        link = a_tag.get('href')
                        
                        # L·∫•y ng√†y ·∫©n
                        date_div = cell.select_one('.date-pdf')
                        date_str = date_div.get_text(strip=True) if date_div else str(current_year)
                        
                        col_name = headers_text[i] if i < len(headers_text) else f"Qu√Ω {i}"
                        full_title = f"{row_title} - {col_name}"
                        
                        if not link: continue
                        if link in seen_ids: continue
                        if any(x['id'] == link for x in new_items): continue

                        new_items.append({
                            "source": f"VIX - {source['name']}",
                            "id": link,
                            "title": full_title,
                            "date": date_str,
                            "link": link
                        })

            # --- X·ª¨ L√ù URL 2: ƒêHƒêCƒê (B·∫¢NG TR·ª∞C TI·∫æP) ---
            elif source["type"] == "DIRECT_TABLE":
                # T√¨m b·∫£ng c√≥ id="tblPublish" (D·ª±a tr√™n ·∫£nh image_1593c0.png)
                table = soup.find(id="tblPublish")
                if not table:
                    # Fallback: T√¨m theo class n·∫øu ID ƒë·ªïi
                    table = soup.select_one('.table-report')
                
                if not table: 
                    # print("   -> Kh√¥ng t√¨m th·∫•y b·∫£ng d·ªØ li·ªáu.")
                    continue

                rows = table.select('tbody tr')
                for tr in rows:
                    # 1. T√åM TI√äU ƒê·ªÄ & LINK
                    # Class: .bic-report__title a
                    title_div = tr.select_one('.bic-report__title a')
                    if not title_div: continue
                    
                    title = title_div.get_text(strip=True)
                    link = title_div.get('href')
                    
                    # 2. T√åM NG√ÄY TH√ÅNG
                    # Class: .bic-report__date (VD: 28/11/2025)
                    date_div = tr.select_one('.bic-report__date')
                    if not date_div: continue
                    
                    date_str = date_div.get_text(strip=True)
                    
                    # 3. L·ªåC NƒÇM
                    try:
                        pub_date = datetime.strptime(date_str, "%d/%m/%Y")
                        if pub_date.year != current_year:
                            continue
                    except:
                        continue # L·ªói ng√†y -> B·ªè qua
                        
                    if not link: continue
                    
                    # 4. CHECK TR√ôNG
                    if link in seen_ids: continue
                    if any(x['id'] == link for x in new_items): continue

                    new_items.append({
                        "source": f"VIX - {source['name']}",
                        "id": link,
                        "title": title,
                        "date": date_str,
                        "link": link
                    })

            time.sleep(0.5)

        except Exception as e:
            print(f"[VIX] L·ªói t·∫°i {source['name']}: {e}")
            continue

    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_dgc_news(seen_ids):
    """
    H√†m c√†o H√≥a ch·∫•t ƒê·ª©c Giang (DGC).
    - URL 1: ƒêHƒêCƒê.
    - URL 2: BCTC (L·ªçc b·ªè b·∫£n English).
    - X·ª≠ l√Ω ng√†y th√°ng d·∫°ng: Day="03", Month="2025, Mar".
    """
    
    current_year = datetime.now().year
    
    configs = [
        {
            "name": "ƒê·∫°i h·ªôi c·ªï ƒë√¥ng",
            "url": "https://ducgiangchem.vn/category/quan-he-co-dong/dai-hoi-co-dong/",
            "filter_english": False
        },
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://ducgiangchem.vn/category/quan-he-co-dong/bao-cao-tai-chinh/",
            "filter_english": True # B·∫≠t ch·∫ø ƒë·ªô l·ªçc b·∫£n ti·∫øng Anh
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t DGC (NƒÉm {current_year}) ---")

    for config in configs:
        # Qu√©t 2 trang ƒë·∫ßu cho ch·∫Øc (d√π th∆∞·ªùng tin m·ªõi ·ªü trang 1)
        # URL ph√¢n trang c·ªßa WordPress: /page/2/
        for page in range(1, 3):
            url = config["url"]
            if page > 1:
                url = f"{config['url']}page/{page}/"
            
            try:
                response = session.get(url, headers=headers, timeout=20, verify=False)
                if response.status_code != 200:
                    # N·∫øu h·∫øt trang (404) th√¨ d·ª´ng
                    break
                    
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # T√¨m c√°c b√†i vi·∫øt (article)
                articles = soup.select('article.type-post')
                
                if not articles: break
                
                count_in_page = 0
                for art in articles:
                    # 1. X·ª¨ L√ù NG√ÄY TH√ÅNG (Ph·ª©c t·∫°p nh·∫•t ·ªü web n√†y)
                    # HTML: <span class="day">03</span> <span class="month">2025, Mar</span>
                    day_tag = art.select_one('.day')
                    month_tag = art.select_one('.month')
                    
                    if not day_tag or not month_tag: continue
                    
                    day_text = day_tag.get_text(strip=True) # "03"
                    month_text = month_tag.get_text(strip=True) # "2025, Mar"
                    
                    # Gh√©p l·∫°i th√†nh chu·ªói: "03 2025, Mar"
                    full_date_str = f"{day_text} {month_text}"
                    
                    try:
                        # Parse ng√†y th√°ng ti·∫øng Anh (%b l√† t√™n th√°ng vi·∫øt t·∫Øt: Jan, Feb, Mar...)
                        pub_date = datetime.strptime(full_date_str, "%d %Y, %b")
                        
                        if pub_date.year != current_year:
                            continue
                        
                        date_display = pub_date.strftime("%d/%m/%Y")
                    except:
                        continue # L·ªói format ng√†y -> B·ªè qua

                    # 2. T√åM TI√äU ƒê·ªÄ & LINK
                    title_tag = art.select_one('.entry-title a')
                    if not title_tag: continue
                    
                    title = title_tag.get_text(strip=True)
                    link = title_tag.get('href')
                    
                    if not link: continue
                    
                    # 3. L·ªåC B·∫¢N TI·∫æNG ANH (Cho m·ª•c BCTC)
                    if config["filter_english"]:
                        title_lower = title.lower()
                        # Lo·∫°i n·∫øu ti√™u ƒë·ªÅ ch·ª©a "(english)" ho·∫∑c "financial statements"
                        if "(english)" in title_lower or "financial statements" in title_lower:
                            continue
                    
                    # 4. CHECK TR√ôNG
                    if link in seen_ids: continue
                    if any(x['id'] == link for x in new_items): continue

                    new_items.append({
                        "source": f"DGC - {config['name']}",
                        "id": link,
                        "title": title,
                        "date": date_display,
                        "link": link
                    })
                    count_in_page += 1
                
                # N·∫øu trang n√†y kh√¥ng c√≥ tin n√†o c·ªßa nƒÉm nay -> D·ª´ng (v√¨ tin x·∫øp theo th·ªùi gian)
                if count_in_page == 0: break
                
                time.sleep(0.5)

            except Exception as e:
                print(f"[DGC] L·ªói t·∫°i {config['name']}: {e}")
                break

    return new_items

def fetch_pow_news(seen_ids):
    """
    H√†m c√†o PV Power (POW).
    - C·∫•u tr√∫c: Grid layout (col-sm-6/12).
    - Ng√†y th√°ng: (dd.mm.yyyy) -> C·∫ßn parse d·∫•u ch·∫•m.
    """
    
    current_year = datetime.now().year
    
    configs = [
        {
            "name": "ƒê·∫°i h·ªôi c·ªï ƒë√¥ng",
            "url": "https://pvpower.vn/vi/tag/dai-hoi-co-dong-23.htm"
        },
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://pvpower.vn/vi/tag/bao-cao-tai-chinh-10.htm"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    
    # S·ª≠ d·ª•ng l·∫°i session v√† adapter t·ª´ code ch√≠nh
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t POW (NƒÉm {current_year}) ---")

    for config in configs:
        try:
            response = session.get(config["url"], headers=headers, timeout=20, verify=False)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m t·∫•t c·∫£ c√°c kh·ªëi tin (wrapper)
            # Class 'post-item-wrapper' bao quanh c·∫£ col-sm-6 v√† col-sm-12
            items = soup.select('.post-item-wrapper')
            
            count_in_page = 0
            
            for item in items:
                # 1. T√åM NG√ÄY TH√ÅNG
                # HTML: <span class="published-date">(25.09.2025)</span>
                date_tag = item.select_one('.published-date')
                if not date_tag: continue
                
                raw_date = date_tag.get_text(strip=True).strip('()') # B·ªè ngo·∫∑c ƒë∆°n
                
                try:
                    # Parse ƒë·ªãnh d·∫°ng dd.mm.yyyy
                    pub_date = datetime.strptime(raw_date, "%d.%m.%Y")
                    
                    if pub_date.year != current_year:
                        continue
                        
                    date_display = pub_date.strftime("%d/%m/%Y")
                except:
                    continue # L·ªói ng√†y -> B·ªè qua

                # 2. T√åM TI√äU ƒê·ªÄ & LINK
                # Ti√™u ƒë·ªÅ n·∫±m trong h2 ho·∫∑c h3 class="title"
                title_tag = item.select_one('.title a')
                if not title_tag: continue
                
                title = title_tag.get_text(strip=True) or title_tag.get('title')
                link = title_tag.get('href')
                
                if not link: continue
                
                # Chu·∫©n h√≥a Link (POW d√πng link t∆∞∆°ng ƒë·ªëi)
                if not link.startswith('http'):
                    link = f"https://pvpower.vn{link}"
                
                # 3. CHECK TR√ôNG
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue

                new_items.append({
                    "source": f"POW - {config['name']}",
                    "id": link,
                    "title": title,
                    "date": date_display,
                    "link": link
                })
                count_in_page += 1
            
            time.sleep(0.5)

        except Exception as e:
            print(f"[POW] L·ªói t·∫°i {config['name']}: {e}")
            continue

    return new_items

def fetch_ree_news(seen_ids):
    """
    H√†m c√†o C∆° ƒëi·ªán l·∫°nh (REE).
    - C·∫•u tr√∫c chu·∫©n: .vii-report-item
    - Ng√†y th√°ng: L·∫•y t·ª´ thu·ªôc t√≠nh datetime="YYYY-MM-DD" c·ªßa th·∫ª <time>.
    """
    
    current_year = datetime.now().year
    
    configs = [
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://www.reecorp.com/danh-muc-bao-cao/bao-cao-tai-chinh/"
        },
        {
            "name": "Ngh·ªã quy·∫øt HƒêQT",
            "url": "https://www.reecorp.com/danh-muc-tai-lieu/nghi-quyet-hdqt/"
        },
        {
            "name": "ƒê·∫°i h·ªôi c·ªï ƒë√¥ng",
            "url": "https://www.reecorp.com/danh-muc-tai-lieu/dai-hoi-co-dong/"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    
    # S·ª≠ d·ª•ng l·∫°i session v√† adapter to√†n c·ª•c
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t REE (NƒÉm {current_year}) ---")

    for config in configs:
        try:
            response = session.get(config["url"], headers=headers, timeout=20, verify=False)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # REE hi·ªÉn th·ªã d·∫°ng list d·ªçc, m·ªói item l√† 1 div class="vii-report-item..."
            # S·ª≠ d·ª•ng select class b·∫Øt ƒë·∫ßu b·∫±ng vii-report-item
            items = soup.select('.vii-report-item')
            
            count_in_page = 0
            
            for item in items:
                # 1. T√åM NG√ÄY TH√ÅNG
                # HTML: <time datetime="2025-10-30">30/10/2025</time>
                time_tag = item.select_one('time')
                if not time_tag: continue
                
                # ∆Øu ti√™n l·∫•y t·ª´ thu·ªôc t√≠nh datetime (chu·∫©n ISO)
                date_iso = time_tag.get('datetime')
                date_text = time_tag.get_text(strip=True)
                
                try:
                    if date_iso:
                        pub_date = datetime.strptime(date_iso, "%Y-%m-%d")
                    else:
                        pub_date = datetime.strptime(date_text, "%d/%m/%Y")
                        
                    if pub_date.year != current_year:
                        continue
                        
                    date_display = pub_date.strftime("%d/%m/%Y")
                except:
                    continue 

                # 2. T√åM TI√äU ƒê·ªÄ
                # HTML: <h3 class="vii-report-item__title">...</h3>
                title_tag = item.select_one('.vii-report-item__title')
                if not title_tag: continue
                title = title_tag.get_text(strip=True)

                # 3. T√åM LINK T·∫¢I
                # HTML: <div class="... download ..."><a href="...">
                download_div = item.select_one('.download a')
                # Fallback: N·∫øu kh√¥ng c√≥ n√∫t download, th·ª≠ l·∫•y n√∫t ebook ho·∫∑c link title
                if not download_div:
                    download_div = item.select_one('.ebook a')
                
                if not download_div: continue
                
                link = download_div.get('href')
                if not link: continue
                
                # 4. CHECK TR√ôNG
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue

                new_items.append({
                    "source": f"REE - {config['name']}",
                    "id": link,
                    "title": title,
                    "date": date_display,
                    "link": link
                })
                count_in_page += 1
            
            time.sleep(0.5)

        except Exception as e:
            print(f"[REE] L·ªói t·∫°i {config['name']}: {e}")
            continue

    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_ocb_news(seen_ids):
    """
    H√†m c√†o OCB (Ng√¢n h√†ng Ph∆∞∆°ng ƒê√¥ng).
    - Ph∆∞∆°ng ph√°p: Extract JSON t·ª´ th·∫ª <script id="serverApp-state">.
    - Logic m·ªõi: D·ª±a tr√™n c·∫•u tr√∫c snippet user cung c·∫•p (c√≥ key 'fileMedia', 'year').
    """
    
    current_year = datetime.now().year
    
    # URL n√†y ch·ª©a c·ª•c JSON to ƒë√πng
    url = "https://www.ocb.com.vn/vi/nha-dau-tu"
    
    # Base URL ƒë·ªÉ gh√©p link PDF (D·ª±a tr√™n domain API trong snippet)
    # L∆∞u √Ω: N·∫øu link 404, c√≥ th·ªÉ th·ª≠ th√™m /reports/ ho·∫∑c /documents/ v√†o sau /uploads/
    base_file_url = "https://webocb-api.ocb.com.vn/uploads/" 
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t OCB (NƒÉm {current_year}) ---")

    try:
        response = session.get(url, headers=headers, timeout=30, verify=False)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. T√åM TH·∫∫ SCRIPT
        script_tag = soup.find('script', id='serverApp-state')
        if not script_tag or not script_tag.string:
            print("[OCB] Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu n·ªÅn.")
            return []
            
        # 2. LOAD JSON
        data_store = json.loads(script_tag.string)
        
        # 3. H√ÄM ƒê·ªÜ QUY T√åM D·ªÆ LI·ªÜU (Scanner)
        # Ch√∫ng ta ƒëi t√¨m m·ªçi dict c√≥ ch·ª©a key 'fileMedia' v√† 'year'
        found_docs = []

        def recursive_search(data):
            if isinstance(data, dict):
                # Ki·ªÉm tra d·∫•u hi·ªáu nh·∫≠n bi·∫øt theo snippet b·∫°n g·ª≠i
                if 'fileMedia' in data and 'name' in data:
                    found_docs.append(data)
                
                # Ti·∫øp t·ª•c ƒë√†o s√¢u v√†o c√°c key con
                for key, value in data.items():
                    recursive_search(value)
            
            elif isinstance(data, list):
                for item in data:
                    recursive_search(item)

        # K√≠ch ho·∫°t h√†m t√¨m ki·∫øm
        recursive_search(data_store)
        
        # 4. X·ª¨ L√ù D·ªÆ LI·ªÜU T√åM ƒê∆Ø·ª¢C
        count_valid = 0
        for item in found_docs:
            # --- L·ªåC NƒÇM ---
            # Snippet c√≥ s·∫µn key "year": 2025 (ki·ªÉu s·ªë int)
            item_year = item.get('year')
            if item_year != current_year:
                continue

            # --- L·∫§Y TH√îNG TIN ---
            title = item.get('name')
            file_name = item.get('fileMedia')
            
            if not title or not file_name: continue
            
            # --- X·ª¨ L√ù NG√ÄY TH√ÅNG ---
            # Snippet: "publishDate": "2025-07-30T00:00:00"
            publish_date = item.get('publishDate')
            date_display = str(current_year)
            
            if publish_date:
                try:
                    # Parse ISO format
                    dt_obj = datetime.fromisoformat(publish_date)
                    date_display = dt_obj.strftime("%d/%m/%Y")
                except:
                    pass

            # --- T·∫†O LINK HO√ÄN CH·ªàNH ---
            full_link = f"{base_file_url}{file_name}"
            
            # --- CHECK TR√ôNG ---
            # D√πng t√™n file l√†m ID v√¨ n√≥ l√† duy nh·∫•t (c√≥ timestamp trong t√™n file)
            news_id = file_name 
            
            if news_id in seen_ids: continue
            if any(x['id'] == news_id for x in new_items): continue

            new_items.append({
                "source": "OCB - Investor JSON",
                "id": news_id,
                "title": title,
                "date": date_display,
                "link": full_link
            })
            count_valid += 1

    except Exception as e:
        print(f"[OCB] L·ªói x·ª≠ l√Ω: {e}")

    return new_items

def fetch_kbc_news(seen_ids):
    """
    H√†m c√†o Kinh B·∫Øc City (KBC).
    - C·∫•u tr√∫c: div.dk-item
    - Ng√†y th√°ng: .dk-item-date (dd/mm/yyyy)
    - Link t·∫£i: ∆Øu ti√™n link trong n√∫t "T·∫£i v·ªÅ" (.btndl-it)
    """
    
    current_year = datetime.now().year
    
    configs = [
        {
            "name": "ƒê·∫°i h·ªôi c·ªï ƒë√¥ng",
            "url": "https://kinhbaccity.vn/dai-hoi-dong-co-dong.htm"
        },
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://kinhbaccity.vn/bao-cao-tai-chinh.htm"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t KBC (NƒÉm {current_year}) ---")

    for config in configs:
        try:
            response = session.get(config["url"], headers=headers, timeout=20, verify=False)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m danh s√°ch c√°c b√†i vi·∫øt
            items = soup.select('.dk-item')
            
            count_in_page = 0
            
            for item in items:
                # 1. T√åM NG√ÄY TH√ÅNG
                # HTML: <div class="dk-item-date">...</i>28/06/2025</div>
                date_div = item.select_one('.dk-item-date')
                if not date_div: continue
                
                date_text = date_div.get_text(strip=True)
                
                try:
                    pub_date = datetime.strptime(date_text, "%d/%m/%Y")
                    if pub_date.year != current_year:
                        continue
                    date_display = pub_date.strftime("%d/%m/%Y")
                except:
                    continue # L·ªói ng√†y -> B·ªè qua

                # 2. T√åM TI√äU ƒê·ªÄ
                # HTML: <h3 class="dk-item-title ..."><a ...>Ti√™u ƒë·ªÅ</a></h3>
                title_tag = item.select_one('.dk-item-title a')
                if not title_tag: continue
                title = title_tag.get_text(strip=True)

                # 3. T√åM LINK T·∫¢I
                # ∆Øu ti√™n n√∫t "T·∫£i v·ªÅ" (Download link)
                # HTML: <a class="btndl-it ..." href="...">
                download_link = item.select_one('.dk-item-desc .btndl-it')
                
                link = ""
                if download_link:
                    link = download_link.get('href')
                else:
                    # Fallback: L·∫•y link t·ª´ ti√™u ƒë·ªÅ n·∫øu kh√¥ng c√≥ n√∫t t·∫£i
                    link = title_tag.get('href')
                
                if not link: continue
                
                # Chu·∫©n h√≥a Link
                if not link.startswith('http'):
                    # KBC ƒë√¥i khi d√πng link t∆∞∆°ng ƒë·ªëi
                    if link.startswith('/'):
                        link = f"https://kinhbaccity.vn{link}"
                    else:
                        link = f"https://kinhbaccity.vn/{link}"
                
                # 4. CHECK TR√ôNG
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue

                new_items.append({
                    "source": f"KBC - {config['name']}",
                    "id": link,
                    "title": title,
                    "date": date_display,
                    "link": link
                })
                count_in_page += 1
            
            time.sleep(0.5)

        except Exception as e:
            print(f"[KBC] L·ªói t·∫°i {config['name']}: {e}")
            continue

    return new_items

def fetch_pnj_news(seen_ids):
    """
    H√†m c√†o PNJ (Phi√™n b·∫£n List-Oriented).
    - ∆Øu ti√™n t√¨m th·∫ª <li> ƒë·ªÉ t√°ch tin (Fix l·ªói d√≠nh Header ch·ªØ ƒëen).
    - Fallback sang c·∫Øt <br> n·∫øu kh√¥ng c√≥ <li>.
    - T·ª± ƒë·ªông s·ª≠a l·ªói nƒÉm (025 -> 2025).
    """
    
    current_year = datetime.now().year
    
    configs = [
        {
            "name": "ƒê·∫°i h·ªôi c·ªï ƒë√¥ng",
            "url": "https://www.pnj.com.vn/quan-he-co-dong/dai-hoi-dong-co-dong/"
        },
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://www.pnj.com.vn/quan-he-co-dong/bao-cao-tai-chinh/"
        },
        {
            "name": "Ngh·ªã quy·∫øt HƒêQT",
            "url": "https://www.pnj.com.vn/quan-he-co-dong/nghi-quyet-cua-hdqt/"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t PNJ (NƒÉm {current_year}) ---")

    for config in configs:
        try:
            response = session.get(config["url"], headers=headers, timeout=20, verify=False)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 1. T√åM KH·ªêI D·ªÆ LI·ªÜU NƒÇM HI·ªÜN T·∫†I
            # T√¨m th·∫ª h2 ch·ª©a "NƒÉm 2025" (ho·∫∑c 2025)
            year_header = soup.find('h2', string=re.compile(str(current_year)))
            if not year_header: 
                # print(f"   [PNJ - {config['name']}] Kh√¥ng th·∫•y m·ª•c NƒÉm {current_year}")
                continue
                
            # T√¨m div n·ªôi dung (class="answer")
            question_div = year_header.find_parent(class_='question')
            if not question_div: continue
            answer_div = question_div.find_next_sibling(class_='answer')
            if not answer_div: continue
            
            # 2. X√ÅC ƒê·ªäNH DANH S√ÅCH ITEM (Logic quan tr·ªçng nh·∫•t)
            # Ki·ªÉm tra xem c√≥ th·∫ª <li> kh√¥ng (nh∆∞ trong snippet b·∫°n g·ª≠i)
            list_items = answer_div.find_all('li')
            
            items_to_process = []
            
            if list_items:
                # [CASE A - ∆ØU TI√äN] N·∫øu c√≥ <li>: Duy·ªát t·ª´ng th·∫ª li. 
                # Header ch·ªØ ƒëen n·∫±m trong th·∫ª <p> b√™n ngo√†i <ol>, n√™n s·∫Ω T·ª∞ ƒê·ªòNG B·ªä LO·∫†I B·ªé.
                items_to_process = list_items
            else:
                # [CASE B - FALLBACK] N·∫øu kh√¥ng c√≥ <li> (d·∫°ng vƒÉn b·∫£n tr√¥i n·ªïi): C·∫Øt chu·ªói theo <br>
                raw_html = answer_div.decode_contents()
                lines = re.split(r'<br\s*/?>', raw_html)
                for line in lines:
                    if line.strip():
                        items_to_process.append(BeautifulSoup(line, 'html.parser'))

            # 3. DUY·ªÜT QUA T·ª™NG ITEM
            count_in_page = 0
            
            for item_soup in items_to_process:
                # --- L·ªåC R√ÅC ---
                # N·∫øu item kh√¥ng c√≥ th·∫ª <a> n√†o -> B·ªè qua ngay
                all_links = item_soup.find_all('a', href=True)
                if not all_links: 
                    continue

                # --- T√åM LINK TI·∫æNG VI·ªÜT ---
                target_link = None
                for a_tag in all_links:
                    txt = a_tag.get_text(strip=True).lower()
                    href = a_tag.get('href')
                    # L·∫•y link kh√¥ng ch·ª©a ch·ªØ "english" trong text v√† href
                    if "english" not in txt and "english" not in href.lower():
                        target_link = href
                        break
                
                if not target_link: continue

                # --- L·∫§Y TEXT ƒê·ªÇ T√åM NG√ÄY V√Ä TITLE ---
                full_text = item_soup.get_text(" ", strip=True)
                
                # Regex t√¨m ng√†y th√°ng: (dd/mm/yyyy) ho·∫∑c (dd/mm/yyy)
                # Ch·∫•p nh·∫≠n nƒÉm c√≥ 3 ho·∫∑c 4 ch·ªØ s·ªë ƒë·ªÉ b·∫Øt l·ªói "025"
                match = re.search(r'\((\d{1,2}/\d{1,2}/\d{3,4})\)', full_text)
                
                date_str = ""
                title = ""
                
                if match:
                    raw_date = match.group(1)
                    # Title l√† ph·∫ßn text TR∆Ø·ªöC ng√†y th√°ng
                    title = full_text[:match.start()].strip(' -:')
                    
                    # Fix l·ªói nƒÉm (VD: 025 -> 2025)
                    parts = raw_date.split('/')
                    if len(parts) == 3:
                        d, m, y = parts
                        if len(y) == 3 and y.startswith('0'): 
                            y = "2" + y # 025 -> 2025
                        date_str = f"{d}/{m}/{y}"
                else:
                    # Fallback n·∫øu kh√¥ng th·∫•y ng√†y trong ngo·∫∑c
                    title = re.sub(r'(T·∫£i v·ªÅ|Xem).*$', '', full_text).strip(' -:')
                    date_str = str(current_year)

                # --- L·ªåC NƒÇM (Final Check) ---
                if str(current_year) not in date_str:
                    continue

                # --- CHU·∫®N H√ìA LINK ---
                if not target_link.startswith('http'):
                    target_link = f"https://www.pnj.com.vn{target_link}"
                if target_link.startswith('//'):
                    target_link = f"https:{target_link}"

                # --- CHECK TR√ôNG ---
                if target_link in seen_ids: continue
                if any(x['id'] == target_link for x in new_items): continue

                new_items.append({
                    "source": f"PNJ - {config['name']}",
                    "id": target_link,
                    "title": title,
                    "date": date_str,
                    "link": target_link
                })
                count_in_page += 1
            
            time.sleep(0.5)

        except Exception as e:
            print(f"[PNJ] L·ªói t·∫°i {config['name']}: {e}")
            continue

    return new_items

def fetch_nvl_news(seen_ids):
    """
    H√†m c√†o Novaland (NVL).
    - C·∫•u tr√∫c: Table chu·∫©n.
    - Ng√†y th√°ng: C·ªôt 2 (td index 1).
    - Link & Title: C·ªôt 3 (td index 2), l·∫•y t·ª´ th·∫ª <a>.
    """
    
    current_year = datetime.now().year
    
    configs = [
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://www.novaland.com.vn/quan-he-dau-tu/cong-bo-thong-tin/bao-cao-tai-chinh"
        },
        {
            "name": "ƒê·∫°i h·ªôi c·ªï ƒë√¥ng",
            # NVL ph√¢n trang ƒêHƒêCƒê theo nƒÉm tr√™n URL
            "url": f"https://www.novaland.com.vn/quan-he-dau-tu/dai-hoi-dong-co-dong/{current_year}"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t NVL (NƒÉm {current_year}) ---")

    for config in configs:
        try:
            response = session.get(config["url"], headers=headers, timeout=20, verify=False)
            
            # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p trang nƒÉm 2025 ch∆∞a t·ªìn t·∫°i (redirect v·ªÅ trang ch·ªß ho·∫∑c l·ªói 404)
            if response.status_code != 200:
                # print(f"   [NVL] Link {config['name']} ch∆∞a c√≥ d·ªØ li·ªáu ho·∫∑c l·ªói.")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m b·∫£ng d·ªØ li·ªáu (class="table")
            table = soup.select_one('table.table')
            if not table: continue
            
            # Duy·ªát c√°c d√≤ng trong tbody
            rows = table.select('tbody tr')
            
            count_in_page = 0
            
            for tr in rows:
                cells = tr.find_all('td')
                if len(cells) < 3: continue
                
                # 1. T√åM NG√ÄY TH√ÅNG (C·ªôt 2)
                date_text = cells[1].get_text(strip=True)
                
                try:
                    pub_date = datetime.strptime(date_text, "%d/%m/%Y")
                    if pub_date.year != current_year:
                        continue
                    date_display = pub_date.strftime("%d/%m/%Y")
                except:
                    continue # L·ªói ng√†y -> B·ªè qua

                # 2. T√åM LINK & TITLE (C·ªôt 3)
                link_tag = cells[2].find('a')
                if not link_tag: continue
                
                link = link_tag.get('href')
                # L·∫•y title t·ª´ thu·ªôc t√≠nh title c·ªßa th·∫ª a (chu·∫©n nh·∫•t theo ·∫£nh)
                title = link_tag.get('title') or link_tag.get_text(strip=True)
                
                if not link: continue
                
                # Chu·∫©n h√≥a Link
                if not link.startswith('http'):
                    link = f"https://www.novaland.com.vn{link}"
                
                # 3. CHECK TR√ôNG
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue

                new_items.append({
                    "source": f"NVL - {config['name']}",
                    "id": link,
                    "title": title,
                    "date": date_display,
                    "link": link
                })
                count_in_page += 1
            
            time.sleep(0.5)

        except Exception as e:
            print(f"[NVL] L·ªói t·∫°i {config['name']}: {e}")
            continue

    return new_items

def fetch_vnd_news(seen_ids):
    """
    H√†m c√†o VNDirect (VND).
    - Link 1, 2: BCTC (X·ª≠ l√Ω ng√†y th√°ng b·ªã chia nh·ªè trong HTML).
    - Link 3: ƒêHƒêCƒê (T√¨m trong sub2congres c·ªßa nƒÉm hi·ªán t·∫°i).
    """
    
    current_year = datetime.now().year
    
    # 1. C·∫•u h√¨nh BCTC
    finance_urls = [
        "https://www.vndirect.com.vn/danh_muc_bao_cao/thong-tin-tai-chinh/?key=bao-cao-tai-chinh-hang-nam",
        "https://www.vndirect.com.vn/danh_muc_bao_cao/thong-tin-tai-chinh/?key=bao-cao-tai-chinh-hang-quy"
    ]
    
    # 2. C·∫•u h√¨nh ƒêHƒêCƒê
    agm_url = "https://www.vndirect.com.vn/dai-hoi-co-dong/"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    
    # S·ª≠ d·ª•ng l·∫°i session v√† adapter c≈©
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t VND (NƒÉm {current_year}) ---")

    # --- PH·∫¶N 1: B√ÅO C√ÅO T√ÄI CH√çNH ---
    for url in finance_urls:
        try:
            response = session.get(url, headers=headers, timeout=20, verify=False)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m c√°c kh·ªëi tin (news-item)
            items = soup.select('.news-item')
            
            for item in items:
                # A. X·ª¨ L√ù NG√ÄY TH√ÅNG (GH√âP CHU·ªñI)
                # C·∫•u tr√∫c: <span class="date-day">14</span>... <p class="date-year">2025</p>
                try:
                    day = item.select_one('.date-day').get_text(strip=True)
                    # Th√°ng n·∫±m trong th·∫ª span k·∫ø ti·∫øp ho·∫∑c sup (t√πy format), l·∫•y text c·ªßa cha ch·ª©a n√≥
                    # C√°ch an to√†n: L·∫•y text c·ªßa div 'news-date' r·ªìi d√πng regex
                    date_div = item.select_one('.news-date')
                    full_date_text = date_div.get_text(" ", strip=True)
                    
                    # Regex t√¨m 3 con s·ªë: ng√†y, th√°ng, nƒÉm
                    nums = re.findall(r'\d+', full_date_text)
                    if len(nums) >= 3:
                        d, m, y = nums[0], nums[1], nums[-1] # NƒÉm th∆∞·ªùng ·ªü cu·ªëi ho·∫∑c class date-year
                        # Check l·∫°i nƒÉm t·ª´ class date-year cho ch·∫Øc
                        year_tag = item.select_one('.date-year')
                        if year_tag: y = year_tag.get_text(strip=True)
                        
                        date_str = f"{d}/{m}/{y}"
                        
                        if int(y) != current_year: continue
                    else:
                        continue
                except:
                    continue

                # B. L·∫§Y TI√äU ƒê·ªÄ & LINK
                title_tag = item.select_one('h3 a')
                if not title_tag: continue
                
                title = title_tag.get_text(strip=True)
                link = title_tag.get('href')
                
                if not link: continue
                
                # Check tr√πng
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue

                new_items.append({
                    "source": "VND - BCTC",
                    "id": link,
                    "title": title,
                    "date": date_str,
                    "link": link
                })

        except Exception as e:
            print(f"[VND-Finance] L·ªói: {e}")

    # --- PH·∫¶N 2: ƒê·∫†I H·ªòI C·ªî ƒê√îNG ---
    try:
        response = session.get(agm_url, headers=headers, timeout=20, verify=False)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # T√¨m t·∫•t c·∫£ c√°c Card (M·ªói nƒÉm/S·ª± ki·ªán l√† 1 card)
        cards = soup.select('.card')
        
        for card in cards:
            # 1. Ki·ªÉm tra Header xem c√≥ ph·∫£i NƒÉm hi·ªán t·∫°i kh√¥ng
            header = card.select_one('.card-header')
            if not header: continue
            
            header_text = header.get_text(strip=True)
            if str(current_year) not in header_text:
                continue # B·ªè qua c√°c nƒÉm c≈©
            
            # 2. T√¨m v√πng "Th√¥ng tin chi ti·∫øt" (class sub2congres)
            # L∆∞u √Ω: sub2congres n·∫±m trong ph·∫ßn collapse
            details_section = card.select_one('.sub2congres')
            if not details_section: continue
            
            # 3. Duy·ªát c√°c d√≤ng tin trong v√πng chi ti·∫øt
            infos = details_section.select('.information')
            
            for info in infos:
                # Link & Title
                a_tag = info.select_one('h6 a')
                if not a_tag: continue
                
                title = a_tag.get_text(strip=True)
                link = a_tag.get('href')
                
                # Date: <p class="font13">12:00 11/09/2025</p>
                date_tag = info.select_one('.font13')
                date_display = str(current_year)
                
                if date_tag:
                    raw_date = date_tag.get_text(strip=True)
                    # Regex b·∫Øt dd/mm/yyyy
                    match = re.search(r'(\d{2}/\d{2}/\d{4})', raw_date)
                    if match:
                        date_display = match.group(1)
                
                if not link: continue
                
                # Check tr√πng
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue

                new_items.append({
                    "source": "VND - ƒêHƒêCƒê",
                    "id": link,
                    "title": title,
                    "date": date_display,
                    "link": link
                })

    except Exception as e:
        print(f"[VND-AGM] L·ªói: {e}")

    return new_items

def fetch_gmd_news(seen_ids):
    """
    H√†m c√†o Gemadept (GMD) - Phi√™n b·∫£n Fix Selector.
    - Chi·∫øn thu·∫≠t: Qu√©t class '.wrap-title' (L√µi ch·ª©a tin) thay v√¨ container b√™n ngo√†i.
    - ƒê·∫£m b·∫£o l·∫•y ƒë·ªß c·∫£ BCTC v√† Th√¥ng b√°o.
    """
    
    current_year = datetime.now().year
    
    configs = [
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://www.gemadept.com.vn/co-dong/bao-cao-tai-chinh/"
        },
        {
            "name": "Th√¥ng b√°o",
            "url": "https://www.gemadept.com.vn/co-dong/thong-bao/"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t GMD (NƒÉm {current_year}) ---")

    for config in configs:
        try:
            response = session.get(config["url"], headers=headers, timeout=20, verify=False)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # --- THAY ƒê·ªîI SELECTOR ---
            # Thay v√¨ t√¨m '.list-info-notify', ta t√¨m th·∫≥ng '.wrap-title'
            # ƒê√¢y l√† class ch·ª©a tr·ª±c ti·∫øp th·∫ª <a>, <h5> (Title) v√† .date
            items = soup.select('.wrap-title')
            
            count_in_page = 0
            
            for item in items:
                # 1. T√åM NG√ÄY TH√ÅNG
                # HTML: <div class="date ...">21.07.2025</div>
                # L∆∞u √Ω: .date n·∫±m b√™n trong .wrap-title (theo ·∫£nh m·ªõi)
                date_div = item.select_one('.date')
                if not date_div: continue
                
                date_text = date_div.get_text(strip=True)
                
                try:
                    # Parse ƒë·ªãnh d·∫°ng: 21.07.2025 (d·∫•u ch·∫•m)
                    pub_date = datetime.strptime(date_text, "%d.%m.%Y")
                    
                    if pub_date.year != current_year:
                        continue
                        
                    date_display = pub_date.strftime("%d/%m/%Y")
                except:
                    continue # L·ªói ng√†y -> B·ªè qua

                # 2. T√åM LINK & TI√äU ƒê·ªÄ
                # HTML: <a href="..."><h5>Ti√™u ƒë·ªÅ</h5>...</a>
                # Th·∫ª a n·∫±m ngay trong .wrap-title ho·∫∑c l√† con tr·ª±c ti·∫øp
                a_tag = item.find('a')
                if not a_tag: continue
                
                link = a_tag.get('href')
                
                # L·∫•y ti√™u ƒë·ªÅ: ∆Øu ti√™n h5, fallback sang text c·ªßa a
                h5_tag = a_tag.find('h5')
                if h5_tag:
                    title = h5_tag.get_text(strip=True)
                else:
                    title = a_tag.get_text(strip=True)
                
                if not link: continue
                
                # Chu·∫©n h√≥a Link
                if not link.startswith('http'):
                    link = f"https://www.gemadept.com.vn{link}"
                
                # 3. CHECK TR√ôNG
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue

                new_items.append({
                    "source": f"GMD - {config['name']}",
                    "id": link,
                    "title": title,
                    "date": date_display,
                    "link": link
                })
                count_in_page += 1
            
            time.sleep(0.5)

        except Exception as e:
            print(f"[GMD] L·ªói t·∫°i {config['name']}: {e}")
            continue

    return new_items

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from datetime import datetime
import time

def fetch_nvb_news(seen_ids):
    """
    H√†m ch·ªâ c√†o BCTC c·ªßa NCB (B·ªè qua c√°c link b·ªã ch·∫∑n).
    """
    current_year = datetime.now().year
    
    # Ch·ªâ gi·ªØ l·∫°i 1 link duy nh·∫•t
    target_url = "https://www.ncb-bank.vn/vi/nha-dau-tu/bao-cao-tai-chinh"
    
    # C·∫•u h√¨nh Selenium
    chrome_options = Options()
    chrome_options.add_argument("--headless=new") # Ch·∫°y ng·∫ßm cho g·ªçn
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36")

    new_items = []
    
    print(f"--- üöÄ Qu√©t NCB (Ch·ªâ BCTC) - NƒÉm {current_year} ---")
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    
    try:
        # print(f"   >> ƒêang truy c·∫≠p: {target_url}...")
        driver.get(target_url)
        
        # 1. Ch·ªù d·ªØ li·ªáu load (Ch·ªù th·∫ª h6 class new-download xu·∫•t hi·ªán)
        try:
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.CLASS_NAME, "new-download"))
            )
            time.sleep(2) # Ch·ªù th√™m ch√∫t cho ch·∫Øc
        except:
            print("      ‚ö†Ô∏è Timeout: Kh√¥ng th·∫•y d·ªØ li·ªáu BCTC.")
            return []

        # 2. L·∫•y HTML & Parse
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        items = soup.find_all('h6', class_='new-download')
        
        count_found = 0
        for item in items:
            # L·∫•y Link & Title
            a_tag = item.find('a')
            if not a_tag: continue
            
            link = a_tag.get('href')
            title = a_tag.get('title') or a_tag.get_text(strip=True)
            
            # L·∫•y Ng√†y (th·∫ª p ngay b√™n c·∫°nh)
            p_tag = item.find('p')
            date_str = str(current_year)
            
            if p_tag:
                raw_date = p_tag.get_text(strip=True) # VD: 28/10/2025 09:49:00
                try:
                    # Parse ng√†y
                    clean_date = raw_date.strip()[:10] # L·∫•y 10 k√Ω t·ª± ƒë·∫ßu (dd/mm/yyyy)
                    dt = datetime.strptime(clean_date, "%d/%m/%Y")
                    
                    if dt.year != current_year:
                        continue # B·ªè qua nƒÉm c≈©
                    date_str = dt.strftime("%d/%m/%Y")
                except:
                    # N·∫øu l·ªói parse nh∆∞ng c√≥ text nƒÉm hi·ªán t·∫°i th√¨ v·∫´n l·∫•y (fallback)
                    if str(current_year) not in raw_date:
                        continue

            # --- L·ªåC TI·∫æNG VI·ªÜT (QUAN TR·ªåNG) ---
            # B·ªè qua c√°c file ti·∫øng Anh
            keywords_en = ["financial report", "statement", "separate", "consolidated", "explanation"]
            if any(kw in title.lower() for kw in keywords_en): 
                continue

            # Chu·∫©n h√≥a Link
            if link and not link.startswith('http'):
                link = f"https://www.ncb-bank.vn{link}"
            
            # Check tr√πng & L∆∞u
            if link not in seen_ids:
                if not any(x['id'] == link for x in new_items):
                    new_items.append({
                        "source": "NCB - BCTC",
                        "id": link,
                        "title": title,
                        "date": date_str,
                        "link": link
                    })
                    count_found += 1
        
        # print(f"      -> T√¨m th·∫•y {count_found} b√°o c√°o m·ªõi.")

    except Exception as e:
        print(f"      ‚ùå L·ªói NCB: {e}")

    finally:
        driver.quit()

    return new_items

def fetch_frt_news(seen_ids):
    """
    H√†m c√†o FPT Retail (FRT) - Phi√™n b·∫£n Selenium API.
    - D√πng tr√¨nh duy·ªát th·∫≠t ƒë·ªÉ m·ªü link API -> Bypass 403 TLS Fingerprint.
    - L·∫•y n·ªôi dung text t·ª´ body tr√¨nh duy·ªát (ch√≠nh l√† chu·ªói JSON) ƒë·ªÉ parse.
    """
    
    current_year = datetime.now().year
    
    configs = [
        {
            "name": "B√°o c√°o t√†i ch√≠nh", 
            "url": "https://api.frt.vn/common/frt-new/api/v1/reports?categoryId=56&locale=vi&page=1&pageSize=10"
        },
        {
            "name": "C√¥ng b·ªë th√¥ng tin", 
            "url": "https://api.frt.vn/common/frt-new/api/v1/reports?categoryId=54&locale=vi&page=1&pageSize=10"
        },
        {
            "name": "ƒê·∫°i h·ªôi c·ªï ƒë√¥ng", 
            "url": "https://api.frt.vn/common/frt-new/api/v1/reports?categoryId=58&locale=vi&page=1&pageSize=10"
        }
    ]

    new_items = []

    # --- C·∫§U H√åNH SELENIUM ---
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    # Fake User-Agent x·ªãn
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t FRT (Selenium Mode - NƒÉm {current_year}) ---")
    
    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    except Exception as e:
        print(f"[FRT] L·ªói driver: {e}")
        return []

    try:
        for config in configs:
            try:
                # M·ªü link API b·∫±ng tr√¨nh duy·ªát
                driver.get(config["url"])
                
                # L·∫•y to√†n b·ªô text trong th·∫ª body (Ch√≠nh l√† chu·ªói JSON th√¥)
                json_text = driver.find_element(By.TAG_NAME, "body").text
                
                # Parse chu·ªói th√†nh Dictionary
                try:
                    json_data = json.loads(json_text)
                except:
                    # print(f"   [FRT] Kh√¥ng ph·∫£i JSON h·ª£p l·ªá t·∫°i {config['name']}")
                    continue

                # --- X·ª¨ L√ù D·ªÆ LI·ªÜU (Logic c≈©) ---
                results = json_data.get('data', {}).get('results', [])
                
                if not results: continue
                
                count_in_cat = 0
                for item in results:
                    attrs = item.get('attributes', {})
                    title = attrs.get('name')
                    
                    # 1. X·ª¨ L√ù NG√ÄY TH√ÅNG
                    date_iso = attrs.get('updatedAt') or attrs.get('createdAt')
                    date_str = str(current_year)
                    
                    if date_iso:
                        try:
                            date_part = date_iso[:10]
                            pub_date = datetime.strptime(date_part, "%Y-%m-%d")
                            if pub_date.year != current_year:
                                continue
                            date_str = pub_date.strftime("%d/%m/%Y")
                        except: pass

                    # 2. L·∫§Y FILE PDF
                    file_data = attrs.get('file', {}).get('data')
                    if not file_data: continue
                        
                    file_attrs = file_data.get('attributes', {})
                    link = file_attrs.get('url')
                    file_name = file_attrs.get('name')
                    
                    if not link: continue
                    
                    # 3. CHU·∫®N H√ìA LINK
                    if not link.startswith('http'):
                        link = f"https://cdn.frt.vn{link}" if not link.startswith('//') else f"https:{link}"

                    # 4. CHECK TR√ôNG
                    news_id = file_name if file_name else link
                    
                    if news_id in seen_ids: continue
                    if any(x['id'] == news_id for x in new_items): continue

                    new_items.append({
                        "source": f"FRT - {config['name']}",
                        "id": news_id,
                        "title": title,
                        "date": date_str,
                        "link": link
                    })
                    count_in_cat += 1
                
                # print(f"   > {config['name']}: L·∫•y ƒë∆∞·ª£c {count_in_cat} tin.")
                time.sleep(1)

            except Exception as e:
                print(f"[FRT] L·ªói x·ª≠ l√Ω {config['name']}: {e}")
                continue
    finally:
        driver.quit()

    return new_items

def fetch_nab_news(seen_ids):
    """
    H√†m c√†o Nam A Bank (NAB) - Phi√™n b·∫£n Stealth Mode.
    - Che gi·∫•u d·∫•u hi·ªáu Bot c·ªßa Selenium ƒë·ªÉ v∆∞·ª£t qua WAF.
    - Logic t√¨m tin: Qu√©t trong .main-list.
    - Logic ng√†y th√°ng: X·ª≠ l√Ω linh ho·∫°t (trong ngo·∫∑c vu√¥ng ho·∫∑c trong ti√™u ƒë·ªÅ).
    """
    
    current_year = datetime.now().year
    
    configs = [
        {
            "name": "B√°o c√°o & C√¥ng b·ªë",
            "url": "https://www.namabank.com.vn/2025-1"
        },
        {
            "name": "ƒê·∫°i h·ªôi c·ªï ƒë√¥ng",
            "url": "https://www.namabank.com.vn/2025-3"
        }
    ]

    new_items = []

    # --- C·∫§U H√åNH SELENIUM CHE D·∫§U V·∫æT ---
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    # T·∫Øt d√≤ng th√¥ng b√°o "Chrome is being controlled..."
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    # Fake User-Agent x·ªãn
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36")
    
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t NAB (Stealth Selenium - NƒÉm {current_year}) ---")
    
    driver = None
    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        # Hack th√™m ƒë·ªÉ che gi·∫•u thu·ªôc t√≠nh webdriver
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    except Exception as e:
        print(f"[NAB] L·ªói driver: {e}")
        return []

    try:
        for config in configs:
            try:
                driver.get(config["url"])
                time.sleep(3) # Ch·ªù load JS
                
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                
                # T√¨m v√πng ch·ª©a danh s√°ch tin
                main_list = soup.select_one('.main-list')
                if not main_list:
                    # print(f"   [NAB] Kh√¥ng th·∫•y .main-list t·∫°i {config['name']}")
                    continue
                
                # T√¨m c√°c item b√†i vi·∫øt (th∆∞·ªùng l√† col-md-6 item ho·∫∑c col-md-6)
                items = main_list.select('.item')
                if not items:
                    # Fallback t√¨m class col-md-6 n·∫øu class item b·ªã thi·∫øu
                    items = main_list.select('.col-md-6')

                count_in_page = 0
                for item in items:
                    # T√¨m link trong figcaption ho·∫∑c icon
                    a_tag = item.select_one('.figcaption a') or item.select_one('.icon a') or item.find('a')
                    if not a_tag: continue
                    
                    link = a_tag.get('href')
                    # Title: ∆Øu ti√™n attribute title > text c·ªßa a
                    raw_title = a_tag.get('title') or a_tag.get_text(strip=True)
                    
                    if not link or not raw_title: continue

                    # --- X·ª¨ L√ù D·ªÆ LI·ªÜU ---
                    clean_title = raw_title
                    date_str = ""
                    
                    # Case 1: C√≥ ng√†y trong ngo·∫∑c vu√¥ng [ƒêƒÉng ng√†y 29/03/2025]...
                    bracket_match = re.search(r'\[.*(\d{1,2}/\d{1,2}/\d{4}).*\]', raw_title)
                    
                    if bracket_match:
                        raw_date = bracket_match.group(1) # L·∫•y ph·∫ßn ng√†y
                        try:
                            pub_date = datetime.strptime(raw_date, "%d/%m/%Y")
                            if pub_date.year != current_year:
                                continue # B·ªè qua nƒÉm c≈©
                            date_str = raw_date
                        except: pass
                        
                        # X√≥a ph·∫ßn ngo·∫∑c vu√¥ng kh·ªèi ti√™u ƒë·ªÅ
                        clean_title = re.sub(r'\[.*?\]', '', raw_title).strip()
                    
                    # Case 2: Kh√¥ng c√≥ ngo·∫∑c vu√¥ng (th∆∞·ªùng l√† BCTC), check nƒÉm trong Title
                    else:
                        if str(current_year) in raw_title:
                            date_str = str(current_year)
                        else:
                            # N·∫øu URL c≈©ng kh√¥ng ch·ª©a nƒÉm hi·ªán t·∫°i th√¨ b·ªè qua (v√¨ link web l√† 2025-x n√™n kh√° an to√†n)
                            if str(current_year) not in config['url']:
                                continue
                            date_str = str(current_year) # Fallback l·∫•y theo URL

                    # --- [M·ªöI] L·ªåC TI·∫æNG ANH ---
                    # Ki·ªÉm tra ti√™u ƒë·ªÅ c√≥ ch·ª©a t·ª´ kh√≥a ti·∫øng Anh kh√¥ng
                    title_upper = clean_title.upper()
                    if "TI·∫æNG ANH" in title_upper or "ENGLISH" in title_upper:
                        continue

                    # --- CHU·∫®N H√ìA LINK ---
                    if not link.startswith('http'):
                        link = f"https://www.namabank.com.vn{link}"
                    
                    # --- CHECK TR√ôNG ---
                    if link in seen_ids: continue
                    if any(x['id'] == link for x in new_items): continue

                    new_items.append({
                        "source": f"NAB - {config['name']}",
                        "id": link,
                        "title": clean_title,
                        "date": date_str,
                        "link": link
                    })
                    count_in_page += 1
                
                # print(f"   > {config['name']}: L·∫•y ƒë∆∞·ª£c {count_in_page} tin.")

            except Exception as e:
                print(f"[NAB] L·ªói x·ª≠ l√Ω {config['name']}: {e}")
                continue
    finally:
        if driver: driver.quit()

    return new_items

def fetch_vci_news(seen_ids):
    """
    H√†m c√†o Vietcap (VCI).
    - C·∫•u tr√∫c: Th·∫ª <a> class="listing-item".
    - Web tƒ©nh (Astro), t·ªëc ƒë·ªô ph·∫£n h·ªìi r·∫•t nhanh.
    """
    
    current_year = datetime.now().year
    
    configs = [
        {
            "name": "Th√¥ng tin c·ªï ƒë√¥ng",
            "url": "https://www.vietcap.com.vn/quan-he-co-dong/thong-tin-co-dong"
        },
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://www.vietcap.com.vn/quan-he-co-dong/bao-cao-tai-chinh"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t VCI (NƒÉm {current_year}) ---")

    for config in configs:
        try:
            response = session.get(config["url"], headers=headers, timeout=20, verify=False)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m danh s√°ch c√°c b√†i vi·∫øt (th·∫ª a c√≥ class listing-item)
            items = soup.select('a.listing-item')
            
            count_in_page = 0
            
            for item in items:
                # 1. T√åM NG√ÄY TH√ÅNG
                # HTML: <div class="date-desktop ...">07/11/2025</div>
                date_div = item.select_one('.date-desktop')
                if not date_div: continue
                
                date_text = date_div.get_text(strip=True)
                
                try:
                    pub_date = datetime.strptime(date_text, "%d/%m/%Y")
                    if pub_date.year != current_year:
                        continue
                    date_display = pub_date.strftime("%d/%m/%Y")
                except:
                    continue 

                # 2. T√åM LINK & TI√äU ƒê·ªÄ
                link = item.get('href')
                
                # Title n·∫±m trong span class="title"
                title_span = item.select_one('.title')
                if not title_span: continue
                
                # ∆Øu ti√™n l·∫•y t·ª´ attribute 'title' c·ªßa span ƒë·ªÉ ƒë∆∞·ª£c text ƒë·∫ßy ƒë·ªß (tr√°nh b·ªã c·∫Øt d√≤ng)
                title = title_span.get('title') or title_span.get_text(strip=True)
                
                if not link: continue
                
                # Chu·∫©n h√≥a Link (VCI d√πng link t∆∞∆°ng ƒë·ªëi)
                if not link.startswith('http'):
                    link = f"https://www.vietcap.com.vn{link}"
                
                # 3. CHECK TR√ôNG
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue

                new_items.append({
                    "source": f"VCI - {config['name']}",
                    "id": link,
                    "title": title,
                    "date": date_display,
                    "link": link
                })
                count_in_page += 1
            
            time.sleep(0.5)

        except Exception as e:
            print(f"[VCI] L·ªói t·∫°i {config['name']}: {e}")
            continue

    return new_items

def fetch_hcm_news(seen_ids):
    """
    H√†m c√†o Ch·ª©ng kho√°n HSC (HCM) - Phi√™n b·∫£n Fix BCTC.
    - BCTC: T√¨m th·∫ª <a> ch·ª©a class 'text-body2-mobile' (Ng√†y) v√† 'text-heading2-mobile' (Ti√™u ƒë·ªÅ).
    - ƒêHƒêCƒê: T√¨m theo Accordion nƒÉm hi·ªán t·∫°i (Gi·ªØ nguy√™n logic c≈© v√¨ ƒë√£ ch·∫°y t·ªët).
    """
    
    current_year = datetime.now().year
    
    configs = [
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://www.hsc.com.vn/vi/tai-chinh/bao-cao-tai-chinh",
            "type": "FINANCE"
        },
        {
            "name": "ƒê·∫°i h·ªôi c·ªï ƒë√¥ng",
            "url": "https://www.hsc.com.vn/vi/dai-hoi-co-dong",
            "type": "AGM"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t HSC (NƒÉm {current_year}) ---")

    for config in configs:
        try:
            response = session.get(config["url"], headers=headers, timeout=20, verify=False)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # --- X·ª¨ L√ù 1: B√ÅO C√ÅO T√ÄI CH√çNH (LOGIC M·ªöI) ---
            if config["type"] == "FINANCE":
                # T√¨m t·∫•t c·∫£ th·∫ª <a> c√≥ href
                all_links = soup.find_all('a', href=True)
                
                for link_tag in all_links:
                    # Ki·ªÉm tra xem trong th·∫ª a n√†y c√≥ ch·ª©a class ng√†y v√† ti√™u ƒë·ªÅ kh√¥ng
                    # L∆∞u √Ω: Class c·ªßa Tailwind r·∫•t d√†i, ta ch·ªâ check t·ª´ kh√≥a ƒë·∫∑c tr∆∞ng
                    date_elem = link_tag.find(class_=lambda x: x and 'text-body2-mobile' in x)
                    title_elem = link_tag.find(class_=lambda x: x and 'text-heading2-mobile' in x)
                    
                    if not date_elem or not title_elem:
                        continue
                        
                    # 1. Parse Ng√†y (text-body2-mobile)
                    date_text = date_elem.get_text(strip=True) # VD: 18.04.2025
                    try:
                        pub_date = datetime.strptime(date_text, "%d.%m.%Y")
                        if pub_date.year != current_year:
                            continue
                        date_display = pub_date.strftime("%d/%m/%Y")
                    except:
                        continue # L·ªói ng√†y -> B·ªè qua

                    # 2. L·∫•y Ti√™u ƒë·ªÅ & Link
                    title = title_elem.get_text(strip=True)
                    link = link_tag.get('href')
                    
                    # 3. Check Tr√πng & L∆∞u
                    if link in seen_ids: continue
                    if any(x['id'] == link for x in new_items): continue

                    new_items.append({
                        "source": f"HSC - {config['name']}",
                        "id": link,
                        "title": title,
                        "date": date_display,
                        "link": link
                    })

            # --- X·ª¨ L√ù 2: ƒê·∫†I H·ªòI C·ªî ƒê√îNG (LOGIC C≈® - ƒê√É T·ªêT) ---
            elif config["type"] == "AGM":
                # T√¨m Header "ƒê·∫°i h·ªôi c·ªï ƒë√¥ng th∆∞·ªùng ni√™n nƒÉm 2025"
                year_keyword = f"ƒê·∫°i h·ªôi c·ªï ƒë√¥ng th∆∞·ªùng ni√™n nƒÉm {current_year}"
                header = soup.find(string=lambda x: x and year_keyword in x)
                
                if not header: continue
                
                header_elem = header.parent
                # T√¨m kh·ªëi bao quanh (Accordion Item)
                accordion_item = header_elem.find_parent(class_=lambda x: x and ("border-b" in x or "flex-col" in x))
                
                if not accordion_item: continue
                
                # T√¨m t·∫•t c·∫£ link trong kh·ªëi ƒë√≥
                # L·ªçc k·ªπ h∆°n: Ch·ªâ l·∫•y link c√≥ href ch·ª©a file ho·∫∑c googleapis
                all_links = accordion_item.find_all('a', href=True)
                
                for a_tag in all_links:
                    link = a_tag.get('href')
                    title = a_tag.get_text(strip=True)
                    
                    # L·∫•y text sibling n·∫øu title trong th·∫ª a qu√° ng·∫Øn (icon)
                    if len(title) < 5:
                        sibling = a_tag.find_previous_sibling() or a_tag.parent.find_previous_sibling()
                        if sibling: title = sibling.get_text(strip=True)
                    
                    # L·ªçc r√°c
                    if "mailto:" in link or "tel:" in link: continue
                    valid_ext = ('.pdf', '.doc', '.docx', 'googleapis.com')
                    if not any(ext in link.lower() for ext in valid_ext): continue

                    if link in seen_ids: continue
                    if any(x['id'] == link for x in new_items): continue

                    new_items.append({
                        "source": f"HSC - {config['name']}",
                        "id": link,
                        "title": title,
                        "date": str(current_year),
                        "link": link
                    })

            time.sleep(0.5)

        except Exception as e:
            print(f"[HSC] L·ªói t·∫°i {config['name']}: {e}")
            continue

    return new_items

def fetch_ksv_news(seen_ids):
    """
    H√†m c√†o Vimico (KSV).
    - C·∫•u tr√∫c: div.post.clearfix -> h2.title a
    - ƒê·∫∑c ƒëi·ªÉm: Kh√¥ng c√≥ ng√†y th√°ng b√™n ngo√†i -> L·∫•y top tin m·ªõi nh·∫•t + L·ªçc theo nƒÉm trong Title.
    """
    
    current_year = datetime.now().year
    url = "https://vimico.vn/cong-bo-thong-tin/"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t KSV (NƒÉm {current_year}) ---")

    try:
        response = session.get(url, headers=headers, timeout=20, verify=False)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # T√¨m danh s√°ch c√°c b√†i vi·∫øt
        items = soup.select('.post.clearfix')
        
        count_in_page = 0
        
        for item in items:
            # 1. T√åM LINK & TI√äU ƒê·ªÄ
            # HTML: <h2 class="title"><a href="...">Ti√™u ƒë·ªÅ...</a></h2>
            title_tag = item.select_one('h2.title a')
            if not title_tag: continue
            
            title = title_tag.get_text(strip=True)
            link = title_tag.get('href')
            
            if not link: continue
            
            # 2. X·ª¨ L√ù NG√ÄY TH√ÅNG (Gi·∫£ l·∫≠p)
            # V√¨ web kh√¥ng hi·ªán ng√†y, ta d√πng chi·∫øn thu·∫≠t:
            # - N·∫øu Title ch·ª©a "2025" -> L·∫•y ch·∫Øc ch·∫Øn.
            # - N·∫øu kh√¥ng, ch·ªâ l·∫•y n·∫øu n√≥ n·∫±m trong Top 5 tin ƒë·∫ßu ti√™n (gi·∫£ ƒë·ªãnh l√† tin m·ªõi).
            
            date_str = str(current_year) # M·∫∑c ƒë·ªãnh nƒÉm nay
            
            is_relevant = False
            if str(current_year) in title:
                is_relevant = True
            elif count_in_page < 5: # L·∫•y 5 tin ƒë·∫ßu ti√™n d√π kh√¥ng c√≥ nƒÉm ƒë·ªÉ tr√°nh s√≥t
                is_relevant = True
            
            if not is_relevant: continue

            # Chu·∫©n h√≥a Link
            if not link.startswith('http'):
                link = f"https://vimico.vn{link}"
            
            # 3. CHECK TR√ôNG
            if link in seen_ids: continue
            if any(x['id'] == link for x in new_items): continue

            new_items.append({
                "source": "KSV - C√¥ng b·ªë th√¥ng tin",
                "id": link,
                "title": title,
                "date": date_str,
                "link": link
            })
            count_in_page += 1
            
            # Ch·ªâ l·∫•y t·ªëi ƒëa 10 tin ƒë·ªÉ tr√°nh spam tin c≈©
            if count_in_page >= 10: break

    except Exception as e:
        print(f"[KSV] L·ªói k·∫øt n·ªëi: {e}")

    return new_items

def fetch_hag_news(seen_ids):
    """
    H√†m c√†o HAGL (Phi√™n b·∫£n Chu·∫©n 4 C·ªôt).
    - X·ª≠ l√Ω giao di·ªán 2025: B·∫£ng danh s√°ch 4 c·ªôt (N·ªôi dung | NƒÉm | Danh m·ª•c | Link/Ng√†y).
    - V·∫´n gi·ªØ fallback cho d·∫°ng Grid c≈© n·∫øu c√≥.
    """
    
    current_year = datetime.now().year
    url = "https://www.hagl.com.vn/co-dong"
    
    # C√°c section c·∫ßn qu√©t
    target_sections = [
        {"id": "section-table-2", "name": "B√°o c√°o t√†i ch√≠nh"},
        {"id": "section-table-4", "name": "Ngh·ªã quy·∫øt HƒêQT"},
        {"id": "section-table-5", "name": "ƒê·∫°i h·ªôi c·ªï ƒë√¥ng"}
    ]

    new_items = []

    # --- C·∫§U H√åNH SELENIUM ---
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36")
    
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t HAG (NƒÉm {current_year}) ---")
    
    driver = None
    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        driver.set_page_load_timeout(60)
        
        driver.get(url)
        # Ch·ªù b·∫£ng load (quan tr·ªçng)
        try:
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.ID, "section-table-2"))
            )
        except:
            print("[HAG] Timeout ch·ªù b·∫£ng d·ªØ li·ªáu.")

        # Cu·ªôn trang d·∫ßn ƒë·ªÉ k√≠ch ho·∫°t lazy load
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3)

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        for section_config in target_sections:
            sec_id = section_config["id"]
            sec_name = section_config["name"]
            
            section_node = soup.find(id=sec_id)
            if not section_node: continue
            
            table = section_node.find('table')
            if not table: continue
            
            # L·∫•y header ƒë·ªÉ check d·∫°ng Grid (n·∫øu c·∫ßn)
            headers = table.select('thead th')
            is_grid = len(headers) > 4 # N·∫øu > 4 c·ªôt th∆∞·ªùng l√† d·∫°ng Grid (Qu√Ω 1,2,3,4)
            
            rows = table.select('tbody tr')
            
            for tr in rows:
                cells = tr.find_all('td')
                if not cells: continue
                
                # --- CHI·∫æN THU·∫¨T 1: D·∫†NG DANH S√ÅCH 4 C·ªòT (Layout 2025) ---
                # C·∫•u tr√∫c: [Title] [Year] [Category] [Link + Date]
                if len(cells) == 4:
                    # 1. Check C·ªôt NƒÉm (C·ªôt 2 - index 1)
                    year_text = cells[1].get_text(strip=True)
                    if str(current_year) not in year_text:
                        continue 

                    # 2. L·∫•y Title (C·ªôt 1 - index 0)
                    title = cells[0].get_text(strip=True)
                    
                    # 3. L·∫•y Link & Date (C·ªôt 4 - index 3)
                    last_cell = cells[3]
                    
                    # Link
                    a_tag = last_cell.find('a')
                    if not a_tag: continue
                    link = a_tag.get('href')
                    
                    # Date (trong badge)
                    date_str = str(current_year)
                    badge = last_cell.select_one('.badge')
                    if badge:
                        raw_date = badge.get_text(strip=True) # VD: 11/11/2025
                        # Regex b·∫Øt ng√†y
                        match = re.search(r'(\d{1,2}/\d{1,2}/\d{4})', raw_date)
                        if match:
                            date_str = match.group(1)
                    
                    if not link.startswith('http'): link = f"https://www.hagl.com.vn{link}"
                    
                    if link in seen_ids: continue
                    if any(x['id'] == link for x in new_items): continue

                    new_items.append({
                        "source": f"HAG - {sec_name}",
                        "id": link,
                        "title": title,
                        "date": date_str,
                        "link": link
                    })

                # --- CHI·∫æN THU·∫¨T 2: D·∫†NG GRID (C≈©/Fallback) ---
                elif len(cells) > 4: 
                    row_title = cells[0].get_text(strip=True)
                    # Duy·ªát c√°c √¥ Qu√Ω
                    for i, cell in enumerate(cells[1:], start=1):
                        a_tag = cell.find('a')
                        if not a_tag: continue
                        
                        link = a_tag.get('href')
                        
                        # Date badge
                        badge = cell.select_one('.badge')
                        if not badge: continue
                        raw_date = badge.get_text(strip=True)
                        
                        try:
                            pub_date = datetime.strptime(raw_date, "%d/%m/%Y")
                            if pub_date.year != current_year: continue
                            date_str = raw_date
                        except: continue
                        
                        # Gh√©p ti√™u ƒë·ªÅ
                        col_name = headers[i].get_text(strip=True) if i < len(headers) else f"C·ªôt {i}"
                        full_title = f"{row_title} - {col_name}"
                        
                        if not link.startswith('http'): link = f"https://www.hagl.com.vn{link}"
                        if link in seen_ids: continue
                        
                        new_items.append({
                            "source": f"HAG - {sec_name}",
                            "id": link,
                            "title": full_title,
                            "date": date_str,
                            "link": link
                        })

    except Exception as e:
        print(f"[HAG] L·ªói x·ª≠ l√Ω: {e}")
    finally:
        if driver: driver.quit()

    return new_items

def fetch_pdr_news(seen_ids):
    """
    H√†m c√†o Ph√°t ƒê·∫°t (PDR).
    - C·∫•u tr√∫c: .block-record (M·ªói d√≤ng tin l√† 1 block-record).
    - Ng√†y th√°ng: T√¨m text sau ch·ªØ "Ng√†y ban h√†nh".
    - Link: Th·∫ª <a> trong .block-cell.
    """
    
    current_year = datetime.now().year
    
    configs = [
        {
            "name": "Th√¥ng b√°o c·ªï ƒë√¥ng",
            "url": "https://www.phatdat.com.vn/thong-bao-co-dong/"
        },
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://www.phatdat.com.vn/bao-cao-tai-chinh/"
        },
        {
            "name": "T√†i li·ªáu c·ªï ƒë√¥ng",
            "url": "https://www.phatdat.com.vn/tai-lieu-co-dong/"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t PDR (NƒÉm {current_year}) ---")

    for config in configs:
        try:
            response = session.get(config["url"], headers=headers, timeout=20, verify=False)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m danh s√°ch c√°c d√≤ng tin
            # PDR d√πng th·∫ª span class="block-record" cho m·ªói d√≤ng
            records = soup.select('.block-record')
            
            count_in_page = 0
            
            for record in records:
                # 1. T√åM NG√ÄY TH√ÅNG
                # HTML: <span class="block-cell ..."><strong>Ng√†y ban h√†nh</strong> 27/06/2025</span>
                date_tag = record.find('strong', string=re.compile("Ng√†y ban h√†nh"))
                
                if not date_tag: continue
                
                # L·∫•y text c·ªßa th·∫ª cha ch·ª©a n√≥
                full_date_text = date_tag.parent.get_text(strip=True)
                # X√≥a ch·ªØ "Ng√†y ban h√†nh" ƒë·ªÉ l·∫•y ng√†y
                date_text = full_date_text.replace("Ng√†y ban h√†nh", "").strip()
                
                try:
                    pub_date = datetime.strptime(date_text, "%d/%m/%Y")
                    if pub_date.year != current_year:
                        continue
                    date_display = pub_date.strftime("%d/%m/%Y")
                except:
                    continue # L·ªói ng√†y ho·∫∑c d√≤ng header -> B·ªè qua

                # 2. T√åM LINK & TI√äU ƒê·ªÄ
                # T√¨m th·∫ª a trong block-cell (Lo·∫°i tr·ª´ n√∫t download ch·ªâ c√≥ icon)
                # T√¨m th·∫ª a c√≥ text d√†i (Ti√™u ƒë·ªÅ)
                a_tags = record.select('a')
                
                target_link = None
                target_title = ""
                
                for a in a_tags:
                    txt = a.get_text(strip=True)
                    # N·∫øu text d√†i > 5 k√Ω t·ª± -> ƒê√¢y l√† ti√™u ƒë·ªÅ
                    if len(txt) > 5:
                        target_link = a.get('href')
                        target_title = txt
                        break
                
                if not target_link:
                    # Fallback: L·∫•y th·∫ª a ƒë·∫ßu ti√™n n·∫øu kh√¥ng l·ªçc ƒë∆∞·ª£c text
                    if a_tags:
                        target_link = a_tags[0].get('href')
                        target_title = a_tags[0].get_text(strip=True) or "T√†i li·ªáu PDR"

                if not target_link: continue
                
                # Chu·∫©n h√≥a Link
                if not target_link.startswith('http'):
                    target_link = f"https://www.phatdat.com.vn{target_link}"
                
                # 3. CHECK TR√ôNG
                if target_link in seen_ids: continue
                if any(x['id'] == target_link for x in new_items): continue

                new_items.append({
                    "source": f"PDR - {config['name']}",
                    "id": target_link,
                    "title": target_title,
                    "date": date_display,
                    "link": target_link
                })
                count_in_page += 1
            
            time.sleep(0.5)

        except Exception as e:
            print(f"[PDR] L·ªói t·∫°i {config['name']}: {e}")
            continue

    return new_items

def fetch_hag_news(seen_ids):
    """
    H√†m c√†o HAGL (HAG) - Fix l·ªói t√¨m sai th·∫ª ch·ª©a b·∫£ng.
    """
    current_year = str(datetime.now().year)
    # current_year = "2025" # Test c·ª©ng n·∫øu c·∫ßn
    
    url = "https://www.hagl.com.vn/co-dong"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t HAGL (NƒÉm {current_year}) ---")

    try:
        response = session.get(url, headers=headers, timeout=20, verify=False)
        soup = BeautifulSoup(response.text, 'html.parser')

        # Danh s√°ch c√°c m·ª•c c·∫ßn qu√©t v√† ID c·ªßa ti√™u ƒë·ªÅ t∆∞∆°ng ·ª©ng
        # section-table-2: B√°o c√°o t√†i ch√≠nh
        # section-table-5: ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng
        targets = [
            {"name": "BCTC", "header_id": "section-table-2"},
            {"name": "ƒêHƒêCƒê", "header_id": "section-table-5"}
        ]

        for target in targets:
            header = soup.find(id=target["header_id"])
            if not header:
                continue

            # --- LOGIC M·ªöI: T√åM B·∫¢NG K·∫æ TI·∫æP ---
            # T·ª´ ti√™u ƒë·ªÅ h3, t√¨m th·∫ª table xu·∫•t hi·ªán ti·∫øp theo trong HTML
            table = header.find_next("table")
            if not table:
                continue

            rows = table.find_all('tr')
            for row in rows:
                # L·∫•y to√†n b·ªô text trong d√≤ng ƒë·ªÉ ki·ªÉm tra nƒÉm
                row_text = row.get_text()
                
                # Ki·ªÉm tra nƒÉm (2025)
                if current_year not in row_text:
                    continue
                
                # T√¨m link
                a_tag = row.find('a', href=True)
                if not a_tag: continue
                
                link = a_tag.get('href')
                if not link.startswith('http'):
                    link = f"https://www.hagl.com.vn{link}"
                
                # L·∫•y ti√™u ƒë·ªÅ t·ª´ c·ªôt ƒë·∫ßu ti√™n (ho·∫∑c text c·ªßa link)
                cols = row.find_all('td')
                if cols:
                    raw_title = cols[0].get_text(strip=True)
                else:
                    raw_title = a_tag.get_text(strip=True) or target["name"]

                # L·∫•y ng√†y (c·ªë g·∫Øng t√¨m trong c·ªôt cu·ªëi ho·∫∑c badge)
                date_str = current_year
                try:
                    badge = row.find('span', class_='badge')
                    if badge:
                        date_text = badge.get_text(strip=True)
                        if '/' in date_text:
                            date_str = date_text[:10] # 24/08/2025
                except:
                    pass

                # Check tr√πng
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue

                new_items.append({
                    "source": f"HAGL - {target['name']}",
                    "id": link,
                    "title": raw_title,
                    "date": date_str,
                    "link": link
                })

    except Exception as e:
        print(f"[HAGL] L·ªói: {e}")

    return new_items

def fetch_msr_news(seen_ids):
    """
    H√†m c√†o Masan High-Tech Materials (MSR).
    - C·∫•u tr√∫c: .releases-box ch·ª©a Date v√† Content.
    - X·ª≠ l√Ω ƒë·∫∑c bi·ªát: M·ªôt tin c√≥ th·ªÉ c√≥ nhi·ªÅu file ƒë√≠nh k√®m (trong th·∫ª <ol> <li>).
    """
    
    current_year = datetime.now().year
    
    configs = [
        {
            "name": "Th√¥ng tin t√†i ch√≠nh",
            "url": "https://masanhightechmaterials.com/vi/investor_category/thong-tin-tai-chinh/"
        },
        {
            "name": "Th√¥ng b√°o c√¥ng ty",
            "url": "https://masanhightechmaterials.com/vi/investor_category/thong-bao-cong-ty/"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t MSR (NƒÉm {current_year}) ---")

    for config in configs:
        try:
            response = session.get(config["url"], headers=headers, timeout=20, verify=False)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m c√°c kh·ªëi tin (releases-box)
            boxes = soup.select('.releases-box')
            
            count_in_page = 0
            
            for box in boxes:
                # 1. T√åM NG√ÄY TH√ÅNG
                # HTML: <div class="date">... 28/10/2025</div>
                date_div = box.select_one('.date')
                if not date_div: continue
                
                date_text = date_div.get_text(strip=True)
                
                try:
                    pub_date = datetime.strptime(date_text, "%d/%m/%Y")
                    if pub_date.year != current_year:
                        continue
                    date_display = pub_date.strftime("%d/%m/%Y")
                except:
                    continue # L·ªói ng√†y -> B·ªè qua

                # 2. X·ª¨ L√ù LINK & TI√äU ƒê·ªÄ
                # MSR c√≥ 2 d·∫°ng:
                # D·∫°ng A: Link n·∫±m ngay ti√™u ƒë·ªÅ H4
                # D·∫°ng B: Ti√™u ƒë·ªÅ H4 kh√¥ng c√≥ link (ho·∫∑c link r·ªóng), b√™n d∆∞·ªõi c√≥ list <ol> <li> ch·ª©a c√°c file
                
                # T√¨m ti√™u ƒë·ªÅ ch√≠nh
                h4_tag = box.select_one('h4 a')
                main_title = ""
                if h4_tag:
                    main_title = h4_tag.get_text(strip=True)
                    main_link = h4_tag.get('href')
                    
                    # N·∫øu ti√™u ƒë·ªÅ ch√≠nh c√≥ link h·ª£p l·ªá -> L·∫•y lu√¥n
                    if main_link and len(main_link) > 5 and "javascript" not in main_link:
                        if main_link not in seen_ids and not any(x['id'] == main_link for x in new_items):
                            new_items.append({
                                "source": f"MSR - {config['name']}",
                                "id": main_link,
                                "title": main_title,
                                "date": date_display,
                                "link": main_link
                            })
                            count_in_page += 1

                # T√¨m c√°c file ƒë√≠nh k√®m (n·∫øu c√≥)
                sub_links = box.select('ol li a')
                for sub_a in sub_links:
                    sub_href = sub_a.get('href')
                    sub_title = sub_a.get_text(strip=True)
                    
                    if not sub_href: continue
                    
                    # Gh√©p ti√™u ƒë·ªÅ: Ti√™u ƒë·ªÅ ch√≠nh + Ti√™u ƒë·ªÅ ph·ª• (ƒë·ªÉ r√µ nghƒ©a)
                    full_title = f"{main_title}: {sub_title}" if main_title else sub_title
                    
                    # Chu·∫©n h√≥a link
                    if not sub_href.startswith('http'):
                        sub_href = f"https://masanhightechmaterials.com{sub_href}"
                        
                    # Check tr√πng
                    if sub_href in seen_ids: continue
                    if any(x['id'] == sub_href for x in new_items): continue

                    new_items.append({
                        "source": f"MSR - {config['name']}",
                        "id": sub_href,
                        "title": full_title,
                        "date": date_display,
                        "link": sub_href
                    })
                    count_in_page += 1
            
            time.sleep(0.5)

        except Exception as e:
            print(f"[MSR] L·ªói t·∫°i {config['name']}: {e}")
            continue

    return new_items